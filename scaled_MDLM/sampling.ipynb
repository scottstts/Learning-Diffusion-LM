{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64cba14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be352ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We offload training and encoding to the Rust implementation\n",
    "# Here we only need it for loading and decoding tokens\n",
    "\n",
    "import json\n",
    "\n",
    "class SimpleBPE:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocab = {}           # token_id -> token_string\n",
    "        self.special_tokens = {}  # optional: useful for reference\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decode token IDs back to text.\"\"\"\n",
    "        return ''.join(self.vocab[i] for i in token_ids)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Load tokenizer from file.\"\"\"\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        self.vocab = {int(k): v for k, v in data['vocab'].items()}\n",
    "        self.special_tokens = {k: int(v) for k, v in data.get('special_tokens', {}).items()}\n",
    "        print(f\"Tokenizer loaded from {path} (vocab size: {len(self.vocab)})\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7165c247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from ./data/tokenizer_bpe.json (vocab size: 8192)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleBPE().load('./data/tokenizer_bpe.json')\n",
    "\n",
    "# use tokenizer\n",
    "def decode(ids): return tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "067ea664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "block_size = 1024\n",
    "final_vocab_size = len(tokenizer.vocab)\n",
    "n_embd = 768\n",
    "n_head = 12\n",
    "n_blocks = 12\n",
    "MASK_TOKEN = tokenizer.special_tokens.get('<MASK>', 'Not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4c1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time embedding\n",
    "\n",
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t.unsqueeze(1) * emb.unsqueeze(0) * 1000 # [B, 1] * [1, half_dim] = [B, half_dim]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1) # [B, dim]\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d0320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA\n",
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "# torchtune RopE implementation expects input shape [B, T, n_head, head_dim], matches the code below\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.head_dim = n_embd // n_head\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embd, 3*n_embd)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "        self.rope = RotaryPositionalEmbeddings(dim=n_embd // n_head)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.residual_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # atten_c: [n_embd, 3*n_embd];\n",
    "        # x: [B, T, n_embd];\n",
    "        # attn_c(x): [B, T, n_embd]@[n_embd, 3*n_embd]=[B, T, 3*n_embd]\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2) # [B, T, 3*n_embd] ==> 3 * [B, T, n_embd]\n",
    "\n",
    "        q = q.view(B, T, self.n_head, self.head_dim) # [B, T, n_embd] = [B, T, n_head*head_dim] ==> [B, T, n_head, head_dim]\n",
    "        # apply RoPE before transpose\n",
    "        q = self.rope(q)\n",
    "        q = q.transpose(1, 2) # [B, n_head, T, head_dim]\n",
    "        k = k.view(B, T, self.n_head, self.head_dim)\n",
    "        k = self.rope(k)\n",
    "        k = k.transpose(1, 2) # [B, n_head, T, head_dim]\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # k.transpose(-2, -1).shape: [B, n_head, head_dim, T]\n",
    "        # (q @ k.transpose(-2, -1)).shape: [B, n_head, T, head_dim]@[B, n_head, head_dim, T] = [B, n_head, T, T]\n",
    "        attn = (q @ k.transpose(-2, -1)*(1.0 / math.sqrt(self.head_dim))) # [B, n_head, T, T]\n",
    "\n",
    "        # No causal mask\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        out = attn @ v # [B, n_head, T, T]@[B, n_head, T, head_dim] = [B, n_head, T, head_dim]\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # drop out on the residual stream\n",
    "        out = self.residual_dropout(self.c_proj(out)) # [B, T, C]@[C, C]=[B, T, C]\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e23d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU activation function.\n",
    "    \n",
    "    This effectively implements:\n",
    "    SwiGLU(x) = (xW + b) * SiLU(xV + c)\n",
    "    \n",
    "    Where the input is split into two parts: one for the 'value' path\n",
    "    and one for the 'gate' path.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        # Split the input tensor into two halves along the last dimension\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        # Apply SiLU (Swish) to the gate and multiply with the value\n",
    "        return x * F.silu(gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38fcc9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFN\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(n_embd, 8*n_embd)\n",
    "        self.swiglu = SwiGLU()\n",
    "        self.c_proj = nn.Linear(4*n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x) # [B, T, C]@[C, 4*C]=[B, T, 4*C]\n",
    "        x = self.swiglu(x)\n",
    "        x = self.c_proj(x) # [B, T, 4*C]@[4*C, C]=[B, T, C]\n",
    "        x = self.dropout(x) # [B, T, C]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08cba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout=0.1, use_time=True):\n",
    "        super().__init__()\n",
    "        self.use_time = use_time\n",
    "        self.rms_norm_1 = nn.RMSNorm(n_embd)\n",
    "        self.rms_norm_2 = nn.RMSNorm(n_embd)\n",
    "        self.attn = MHA(n_embd, n_head, dropout)\n",
    "        self.ffn = FFN(n_embd, dropout)\n",
    "\n",
    "        # RMSNorm is designed to be \"shift-invariant\" (it centers data around 0), in Diffusion models, injecting the shift (beta) back in after normalization is a powerful way to tell the network about the noise level\n",
    "        if use_time:\n",
    "            self.time_ffn = nn.Sequential(\n",
    "                nn.Linear(n_embd, 2 * n_embd),\n",
    "                SwiGLU(), # SwiGLU will half the feature dimension\n",
    "                nn.Linear(n_embd, 4*n_embd)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, time_emb=None):\n",
    "        if self.use_time and time_emb is not None:\n",
    "            time_params = self.time_ffn(time_emb) # [B, e_embd]@[n_embd, 4*n_embd]=[B, 4*n_embd]\n",
    "            shift1, scale1, shift2, scale2 = time_params.chunk(4, dim=-1) # [B, n_embd]\n",
    "\n",
    "            h = self.rms_norm_1(x) * (1 + scale1.unsqueeze(1)) + shift1.unsqueeze(1) # [B, T, C]*[B, 1, C]+[B, 1, C]=[B, T, C]\n",
    "            x = x + self.attn(h)\n",
    "            h = self.rms_norm_2(x) * (1 + scale2.unsqueeze(1)) + shift2.unsqueeze(1) # [B, T, C]*[B, 1, C]+[B, 1, C]=[B, T, C]\n",
    "            x = x + self.ffn(h) # [B, T, C]\n",
    "        else:\n",
    "            x = x + self.attn(self.rms_norm_1(x))\n",
    "            x = x + self.ffn(self.rms_norm_2(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47bd7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full MDLM\n",
    "\n",
    "class MDLM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            n_embd,\n",
    "            n_head,\n",
    "            n_block,\n",
    "            block_size,\n",
    "            dropout=0.1,\n",
    "            use_time = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.use_time = use_time\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        if use_time:\n",
    "            self.time_emb = SinusoidalTimeEmbedding(n_embd)\n",
    "            self.time_proj = nn.Sequential(\n",
    "                nn.Linear(n_embd, 2*n_embd),\n",
    "                SwiGLU(),\n",
    "                nn.Linear(n_embd, n_embd)\n",
    "            )\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(n_embd, n_head, dropout, use_time) for _ in range(n_block)\n",
    "        ])\n",
    "\n",
    "        self.rms_norm_final = nn.RMSNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        # tie input and output embedding weights\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f'Model has {n_params/1e6:.2f}M parameters.')\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.RMSNorm):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, x, t=None):\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok_emb = self.tok_emb(x) # [B, T, n_embd]\n",
    "        h = self.dropout(tok_emb)\n",
    "\n",
    "        if self.use_time and t is not None:\n",
    "            t_emb = self.time_emb(t) # [B, n_embd]\n",
    "            t_emb = self.time_proj(t_emb) # [B, n_embd]\n",
    "        else:\n",
    "            t_emb = None\n",
    "\n",
    "        for block in self.blocks:\n",
    "            h = block(h, t_emb)\n",
    "\n",
    "        h = self.rms_norm_final(h)\n",
    "        logits = self.lm_head(h) # [B, T, V]\n",
    "\n",
    "        # Make sure model doesn't learn the behavior of predicting a mask token. Aligns with inference\n",
    "        logits[:, :, MASK_TOKEN] = float('-inf')\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c78b895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 163.97M parameters.\n",
      "Loading model weights from ./ckpt/latest.pt...\n",
      "Model loaded successfully (from Epoch 7)\n",
      "Generated samples:\n",
      "============================================================\n",
      "One sunny day, Lily and her mom went to the park. It was a big festival. All many children went there and had lots of fun. Lily was very scared that something bad might happen there.\n",
      "Lily's mom loved going to the festival because there was a little boy there. But her mom told her to be kind to everyone when it was a reason. Suddenly, Lily noticed that the boy was hiding behind a tree. Lily was not sure what to do, so she went back to hide in her room and waited. She heard her mom calling her name.\n",
      "Lily went to the door and saw the little girl looking for her. Her mom had gone to the festival, and it was time to go home. Lily was so happy that she gave the little girl a big hug. She knew it was important because she helped the girl who would have a chance to arrive.\n",
      "When they arrived at the park, everyone danced and clapped their hands and games. They had a great party, and Lily and her mom were happy and glad they were safe. From that day on, Lily knew that that she could be a good friend when she rescued something.\n",
      "\n",
      "\n",
      "\n",
      "Once there was a little boy. The boy had a soft bed. He liked to play on the bed and glow in the sun. One night, he had a scary dream at night. The sky was very dark. The boy was scared because he had made his bed all alone in the dark.\n",
      "He went to the big bed and decided to take the night in and make his bed. Then, he covered it with a soft, warm pillow. With a flash of glow, the boy lay on the big, soft bed. To his surprise, the boy fell asleep, and the sky came out. The little boy felt safe and warm in his playroom again. He knew that his home was his favorite place.\n",
      "From that day on, the little boy always went to his playroom and would glow in the big bed. The stars loved him, and they were very happy to have in the soft bed. And they all lived happily ever after.\n",
      "\n",
      "\n",
      "\n",
      "One day, a little girl named Amy went to the park. She had a big, soft cushion. Amy loved to admire her cushion and she walked around.\n",
      "Later, Amy met her friend, Tim. Tim asked her, \"Your cushion is so pretty!\" Amy said, \"Thank you! I like when you walk in a lot.\" Lucy smiled and said, \"Do you want to play with my cushion?\" Amy said, \"Yes, please!\"\n",
      "Amy then showed her cushion to her friend, Tim. They became good friends and liked to admire new things. They brought their other things to playtime, and now Amy made a new friend, the big, soft dog.\n",
      "\n",
      "\n",
      "\n",
      "Once upon a time, there was a little girl named Lily. Lily was a very organized girl. She loved playing with her toys and making \n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "\n",
    "import os\n",
    "\n",
    "# 1. Define the path (must match your training loop)\n",
    "CKPT_PATH = './ckpt/latest.pt'\n",
    "\n",
    "# 2. Re-initialize the model architecture \n",
    "model = MDLM(\n",
    "    vocab_size=final_vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_block=n_blocks,\n",
    "    block_size=block_size,\n",
    ").to(device)\n",
    "\n",
    "# 3. Load the checkpoint\n",
    "if os.path.exists(CKPT_PATH):\n",
    "    print(f\"Loading model weights from {CKPT_PATH}...\")\n",
    "    # map_location ensures that if you trained on GPU but load on CPU, it works\n",
    "    checkpoint = torch.load(CKPT_PATH, map_location=device, weights_only=False)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith('_orig_mod.'):\n",
    "            # Remove the prefix\n",
    "            new_key = key[10:]  # len('_orig_mod.') == 10\n",
    "            new_state_dict[new_key] = value\n",
    "        else:\n",
    "            new_state_dict[key] = value\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    print(f\"Model loaded successfully (from Epoch {checkpoint['epoch']+1})\")\n",
    "else:\n",
    "    print(f\"Warning: No checkpoint found at {CKPT_PATH}. Using random initialization.\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, seq_len, num_steps=100, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate text using the reverse diffusion process.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained MDLM\n",
    "        seq_len: Length of sequence to generate\n",
    "        num_steps: Number of denoising steps\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Generated token sequence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Start with all masks\n",
    "    x = torch.full((1, seq_len), MASK_TOKEN, dtype=torch.long, device=device)\n",
    "    \n",
    "    # Linearly spaced time steps from 1 to 0\n",
    "    timesteps = torch.linspace(1, 0, num_steps + 1, device=device)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        t_current = timesteps[i]\n",
    "        t_next = timesteps[i + 1]\n",
    "        \n",
    "        # Get model predictions\n",
    "        t_batch = torch.tensor([t_current], device=device)\n",
    "        logits = model(x, t_batch)  # [1, seq_len, vocab_size]\n",
    "        # Model is predicting all positions at each forward pass\n",
    "\n",
    "        if top_p < 1.0:\n",
    "            # Sort logits in descending order\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "            \n",
    "            # Calculate cumulative probabilities\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            \n",
    "            # Remove tokens with cumulative probability above top_p\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            \n",
    "            # handle edge case where top_p is too small\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            # 4. Scatter mask back to original indices\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                dim=-1, index=sorted_indices, src=sorted_indices_to_remove\n",
    "            )\n",
    "            \n",
    "            # 5. Set filtered logits to -inf\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # Don't predict [MASK] token during sampling\n",
    "        logits[:, :, MASK_TOKEN] = float('-inf')\n",
    "        \n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)  # [1, seq_len, vocab_size]\n",
    "        \n",
    "        # Sample predictions for all positions\n",
    "        # (we'll only use some based on remasking strategy)\n",
    "        pred_tokens = torch.multinomial(\n",
    "            probs.view(-1, probs.size(-1)), \n",
    "            num_samples=1\n",
    "        ).view(1, seq_len)\n",
    "        \n",
    "        # Determine which positions to unmask this step (Ancestral Sampling)\n",
    "        # Instead of forcing a specific count (top-k), we roll a probability die for each token\n",
    "        \n",
    "        # 1. Calculate the probability of unmasking at this step\n",
    "        if t_current > 0:\n",
    "            p_unmask = (t_current - t_next) / t_current\n",
    "        else:\n",
    "            p_unmask = 1.0  # Force finish at the very end\n",
    "            \n",
    "        # 2. Roll the dice for every position\n",
    "        # Generate random values [0, 1]. If value < p_unmask, we reveal the token.\n",
    "        random_values = torch.rand_like(probs[:, :, 0]) # Shape: [1, seq_len]\n",
    "        should_unmask = random_values < p_unmask\n",
    "        \n",
    "        # 3. Apply updates\n",
    "        # Only update if it is CURRENTLY a mask AND the dice roll said \"Unmask\"\n",
    "        is_mask = (x == MASK_TOKEN)\n",
    "        update_mask = is_mask & should_unmask\n",
    "        \n",
    "        x[update_mask] = pred_tokens[update_mask]\n",
    "    \n",
    "    # Final cleanup: unmask any remaining masks\n",
    "    is_mask = (x == MASK_TOKEN)\n",
    "    if is_mask.any():\n",
    "        logits = model(x, torch.tensor([0.0], device=device))\n",
    "        logits[:, :, MASK_TOKEN] = float('-inf')\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "        final_preds = probs.argmax(dim=-1)\n",
    "        x[is_mask] = final_preds[is_mask]\n",
    "    \n",
    "    return x[0].tolist()\n",
    "\n",
    "# Generate some samples!\n",
    "print(\"Generated samples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tokens = sample(model, seq_len=1024, num_steps=100, temperature=1.0, top_p=0.9)\n",
    "text = decode(tokens)\n",
    "print(''.join(text).replace('<|endoftext|>', '\\n\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ed5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
