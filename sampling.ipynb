{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64cba14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be352ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "class SimpleBPE:\n",
    "    \"\"\"\n",
    "    Simple Byte Pair Encoding tokenizer.\n",
    "    Trains custom vocabulary on your text data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {}           # token_id -> token_string\n",
    "        self.vocab_inv = {}       # token_string -> token_id\n",
    "        self.merges = []          # list of merge rules\n",
    "        self.special_tokens = {}\n",
    "        self.max_token_len = 1    # for fast encoding\n",
    "        \n",
    "    def _get_stats(self, token_seqs):\n",
    "        \"\"\"Count frequency of adjacent pairs.\"\"\"\n",
    "        pairs = Counter()\n",
    "        for seq in token_seqs:\n",
    "            for i in range(len(seq) - 1):\n",
    "                pairs[(seq[i], seq[i + 1])] += 1\n",
    "        return pairs\n",
    "    \n",
    "    def _merge_pair(self, token_seqs, pair, new_token):\n",
    "        \"\"\"Merge all occurrences of pair into new_token.\"\"\"\n",
    "        new_seqs = []\n",
    "        for seq in token_seqs:\n",
    "            new_seq = []\n",
    "            i = 0\n",
    "            while i < len(seq):\n",
    "                if i < len(seq) - 1 and seq[i] == pair[0] and seq[i + 1] == pair[1]:\n",
    "                    new_seq.append(new_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_seq.append(seq[i])\n",
    "                    i += 1\n",
    "            new_seqs.append(new_seq)\n",
    "        return new_seqs\n",
    "    \n",
    "    def train(self, text, vocab_size=1000, verbose=True):\n",
    "        \"\"\"Train BPE on the given text.\"\"\"\n",
    "        if verbose:\n",
    "            print(f\"Training BPE tokenizer on {len(text):,} characters...\")\n",
    "        \n",
    "        # Split into words (keep whitespace attached)\n",
    "        words = re.findall(r'\\S+|\\s+', text)\n",
    "        token_seqs = [[c for c in word] for word in words]\n",
    "        \n",
    "        # Initial vocabulary = unique characters\n",
    "        chars = sorted(set(text))\n",
    "        self.vocab = {i: c for i, c in enumerate(chars)}\n",
    "        self.vocab_inv = {c: i for i, c in enumerate(chars)}\n",
    "        next_id = len(chars)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Base vocabulary: {len(chars)} characters\")\n",
    "        \n",
    "        # Convert to token IDs\n",
    "        token_seqs = [[self.vocab_inv[c] for c in seq] for seq in token_seqs]\n",
    "        \n",
    "        # Iteratively merge most frequent pairs\n",
    "        self.merges = []\n",
    "        num_merges = vocab_size - len(chars)\n",
    "        \n",
    "        for i in range(num_merges):\n",
    "            stats = self._get_stats(token_seqs)\n",
    "            if not stats:\n",
    "                break\n",
    "                \n",
    "            best_pair = max(stats, key=stats.get)\n",
    "            new_token_str = self.vocab[best_pair[0]] + self.vocab[best_pair[1]]\n",
    "            \n",
    "            self.vocab[next_id] = new_token_str\n",
    "            self.vocab_inv[new_token_str] = next_id\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            token_seqs = self._merge_pair(token_seqs, best_pair, next_id)\n",
    "            \n",
    "            if verbose and (i + 1) % 200 == 0:\n",
    "                print(f\"  {i+1}/{num_merges} merges completed...\")\n",
    "            \n",
    "            next_id += 1\n",
    "        \n",
    "        # Set max token length for fast encoding\n",
    "        self.max_token_len = max(len(t) for t in self.vocab.values())\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Training complete! Vocabulary size: {len(self.vocab)}\")\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def add_special_token(self, token_str):\n",
    "        \"\"\"Add a special token like <MASK>.\"\"\"\n",
    "        token_id = len(self.vocab)\n",
    "        self.vocab[token_id] = token_str\n",
    "        self.vocab_inv[token_str] = token_id\n",
    "        self.special_tokens[token_str] = token_id\n",
    "        self.max_token_len = max(self.max_token_len, len(token_str))\n",
    "        return token_id\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Fast encoding using greedy longest-match.\n",
    "        O(max_token_len * text_len) instead of O(num_merges * text_len)\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        n = len(text)\n",
    "        \n",
    "        while i < n:\n",
    "            # Try longest match first, then shorter\n",
    "            for length in range(min(self.max_token_len, n - i), 0, -1):\n",
    "                substr = text[i:i + length]\n",
    "                if substr in self.vocab_inv:\n",
    "                    tokens.append(self.vocab_inv[substr])\n",
    "                    i += length\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown character at position {i}: {repr(text[i])}\")\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Decode token IDs back to text.\"\"\"\n",
    "        return ''.join(self.vocab[i] for i in token_ids)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save tokenizer to file.\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({\n",
    "                'vocab': {str(k): v for k, v in self.vocab.items()},\n",
    "                'merges': self.merges,\n",
    "                'special_tokens': self.special_tokens,\n",
    "                'max_token_len': self.max_token_len\n",
    "            }, f)\n",
    "        print(f\"Tokenizer saved to {path}\")\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load tokenizer from file.\"\"\"\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        self.vocab = {int(k): v for k, v in data['vocab'].items()}\n",
    "        self.vocab_inv = {v: k for k, v in self.vocab.items()}\n",
    "        self.merges = [tuple(m) for m in data['merges']]\n",
    "        self.special_tokens = data.get('special_tokens', {})\n",
    "        self.max_token_len = data.get('max_token_len', max(len(t) for t in self.vocab.values()))\n",
    "        print(f\"Tokenizer loaded from {path} (vocab size: {len(self.vocab)})\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7165c247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from tokenizer_bpe.json (vocab size: 1001)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleBPE().load('tokenizer_bpe.json')\n",
    "\n",
    "# use tokenizer\n",
    "def decode(ids): return tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "067ea664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "block_size = 512\n",
    "final_vocab_size = len(tokenizer.vocab)\n",
    "batch_size = 16        # Fits in GPU memory (Physical Batch)\n",
    "target_batch_size = 64 # What we want mathematically (Effective Batch)\n",
    "grad_accum_steps = target_batch_size // batch_size # 64 // 16 = 4 steps\n",
    "n_embd = 512\n",
    "n_head = 4\n",
    "n_blocks = 6\n",
    "MASK_TOKEN = tokenizer.special_tokens.get('<MASK>', 'Not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b4c1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time embedding\n",
    "\n",
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    \n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = t.unsqueeze(1) * emb.unsqueeze(0) * 1000 # [B, 1] * [1, half_dim] = [B, half_dim]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1) # [B, dim]\n",
    "\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d0320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA\n",
    "from torchtune.modules import RotaryPositionalEmbeddings\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.head_dim = n_embd // n_head\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embd, 3*n_embd)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "        self.rope = RotaryPositionalEmbeddings(dim=n_embd // n_head)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.residual_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # atten_c: [n_embd, 3*n_embd]; \n",
    "        # x: [B, T, n_embd]; \n",
    "        # attn_c(x): [B, T, n_embd]@[n_embd, 3*n_embd]=[B, T, 3*n_embd]\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2) # [B, T, 3*n_embd] ==> 3 * [B, T, n_embd]\n",
    "\n",
    "        q = q.view(B, T, self.n_head, self.head_dim) # [B, T, n_embd] = [B, T, n_head*head_dim] ==> [B, T, n_head, head_dim]\n",
    "        # apply RoPE before transpose\n",
    "        q = self.rope(q)\n",
    "        q = q.transpose(1, 2) # [B, n_head, T, head_dim]\n",
    "        k = k.view(B, T, self.n_head, self.head_dim)\n",
    "        k = self.rope(k)\n",
    "        k = k.transpose(1, 2) # [B, n_head, T, head_dim]\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # k.transpose(-2, -1).shape: [B, n_head, head_dim, T]\n",
    "        # (q @ k.transpose(-2, -1)).shape: [B, n_head, T, head_dim]@[B, n_head, head_dim, T] = [B, n_head, T, T]\n",
    "        attn = (q @ k.transpose(-2, -1)*(1.0 / math.sqrt(self.head_dim))) # [B, n_head, T, T]\n",
    "        \n",
    "        # No causal mask\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        out = attn @ v # [B, n_head, T, T]@[B, n_head, T, head_dim] = [B, n_head, T, head_dim]\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        # drop out on the residual stream\n",
    "        out = self.residual_dropout(self.c_proj(out)) # [B, T, C]@[C, C]=[B, T, C]\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e23d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU activation function.\n",
    "    \n",
    "    This effectively implements:\n",
    "    SwiGLU(x) = (xW + b) * SiLU(xV + c)\n",
    "    \n",
    "    Where the input is split into two parts: one for the 'value' path\n",
    "    and one for the 'gate' path.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        # Split the input tensor into two halves along the last dimension\n",
    "        x, gate = x.chunk(2, dim=-1)\n",
    "        # Apply SiLU (Swish) to the gate and multiply with the value\n",
    "        return x * F.silu(gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38fcc9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFN\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(n_embd, 8*n_embd)\n",
    "        self.swiglu = SwiGLU()\n",
    "        self.c_proj = nn.Linear(4*n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x) # [B, T, C]@[C, 4*C]=[B, T, 4*C]\n",
    "        x = self.swiglu(x)\n",
    "        x = self.c_proj(x) # [B, T, 4*C]@[4*C, C]=[B, T, C]\n",
    "        x = self.dropout(x) # [B, T, C]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08cba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head, dropout=0.1, use_time=True):\n",
    "        super().__init__()\n",
    "        self.use_time = use_time\n",
    "        self.rms_norm_1 = nn.RMSNorm(n_embd)\n",
    "        self.rms_norm_2 = nn.RMSNorm(n_embd)\n",
    "        self.attn = MHA(n_embd, n_head, dropout)\n",
    "        self.ffn = FFN(n_embd, dropout)\n",
    "\n",
    "        # RMSNorm is designed to be \"shift-invariant\" (it centers data around 0), in Diffusion models, injecting the shift (beta) back in after normalization is a powerful way to tell the network about the noise level\n",
    "        if use_time:\n",
    "            self.time_ffn = nn.Sequential(\n",
    "                nn.Linear(n_embd, 2 * n_embd),\n",
    "                SwiGLU(), # SwiGLU will half the feature dimension\n",
    "                nn.Linear(n_embd, 4*n_embd)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, time_emb=None):\n",
    "        if self.use_time and time_emb is not None:\n",
    "            time_params = self.time_ffn(time_emb) # [B, e_embd]@[n_embd, 4*n_embd]=[B, 4*n_embd]\n",
    "            shift1, scale1, shift2, scale2 = time_params.chunk(4, dim=-1) # [B, n_embd]\n",
    "\n",
    "            h = self.rms_norm_1(x) * (1 + scale1.unsqueeze(1)) + shift1.unsqueeze(1) # [B, T, C]*[B, 1, C]+[B, 1, C]=[B, T, C]\n",
    "            x = x + self.attn(h)\n",
    "            h = self.rms_norm_2(x) * (1 + scale2.unsqueeze(1)) + shift2.unsqueeze(1) # [B, T, C]*[B, 1, C]+[B, 1, C]=[B, T, C]\n",
    "            x = x + self.ffn(h) # [B, T, C]\n",
    "        else:\n",
    "            x = x + self.attn(self.rms_norm_1(x))\n",
    "            x = x + self.ffn(self.rms_norm_2(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47bd7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full MDLM\n",
    "\n",
    "class MDLM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            n_embd,\n",
    "            n_head,\n",
    "            n_block,\n",
    "            block_size,\n",
    "            dropout=0.1,\n",
    "            use_time = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.use_time = use_time\n",
    "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
    "        if use_time:\n",
    "            self.time_emb = SinusoidalTimeEmbedding(n_embd)\n",
    "            self.time_proj = nn.Sequential(\n",
    "                nn.Linear(n_embd, 2*n_embd),\n",
    "                SwiGLU(),\n",
    "                nn.Linear(n_embd, n_embd)\n",
    "            )\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(n_embd, n_head, dropout, use_time) for _ in range(n_block)\n",
    "        ])\n",
    "\n",
    "        self.rms_norm_final = nn.RMSNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "        # tie input and output embedding weights\n",
    "        self.lm_head.weight = self.tok_emb.weight\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f'Model has {n_params/1e6:.2f}M parameters.')\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.RMSNorm):\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, x, t=None):\n",
    "        B, T = x.shape\n",
    "\n",
    "        tok_emb = self.tok_emb(x) # [B, T, n_embd]\n",
    "        h = self.dropout(tok_emb)\n",
    "\n",
    "        if self.use_time and t is not None:\n",
    "            t_emb = self.time_emb(t) # [B, n_embd]\n",
    "            t_emb = self.time_proj(t_emb) # [B, n_embd]\n",
    "        else:\n",
    "            t_emb = None\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            h = block(h, t_emb)\n",
    "\n",
    "        h = self.rms_norm_final(h)\n",
    "        logits = self.lm_head(h) # [B, T, V]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c78b895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 35.97M parameters.\n",
      "Loading model weights from ./ckpt/latest.pt...\n",
      "Model loaded successfully (from Epoch 100)\n",
      "Generated samples:\n",
      "============================================================\n",
      "was a tin face — reliving in piece of report \n",
      "that he had been following cousin every day — the \n",
      "subject that Dudley was grudging laughter and certed \n",
      "him firmly. He was sure that Dudley’s face and sense. They were lowered his \n",
      "angry teeth, further and foggy fell slowly against the ground \n",
      "again as he pointed at Harry as Dudley already gave it \n",
      "again. \n",
      "“See, knowing.” \n",
      "“ARY!” Marge exploded Uncle Vernon, biding \n",
      "of fe again. Dudley his heart became still gasping \n",
      "more bravely. “You’ll see Aunt Petunia till any \n",
      "palciage.” \n",
      "“And they’re quite haven’t kept to buy a stop, \n",
      "coweringling in his stest. \n",
      "“You might be all looking forward into fat clothes. Marge \n",
      "think she needs Dudtilk, stabbling them up.” \n",
      "“We’ll escape they are going to reward Muggles!” ” ( \n",
      "cle Vernon drew Only looking each other over his \n",
      "shoulder), was almost pleasant. Uncle Vernon; Vernon glared \n",
      "her him, dripping over his arm that was much faster into \n",
      "Aunt Petunia and Hundred and Mfe Harry. Go \n",
      "Uncle Vernon stood, playing grouped her mustache, for a long \n",
      "time\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "\n",
    "import os\n",
    "\n",
    "# 1. Define the path (must match your training loop)\n",
    "CKPT_PATH = './ckpt/latest.pt'\n",
    "\n",
    "# 2. Re-initialize the model architecture \n",
    "model = MDLM(\n",
    "    vocab_size=final_vocab_size,\n",
    "    n_embd=n_embd,\n",
    "    n_head=n_head,\n",
    "    n_block=n_blocks,\n",
    "    block_size=block_size,\n",
    ").to(device)\n",
    "\n",
    "# 3. Load the checkpoint\n",
    "if os.path.exists(CKPT_PATH):\n",
    "    print(f\"Loading model weights from {CKPT_PATH}...\")\n",
    "    # map_location ensures that if you trained on GPU but load on CPU, it works\n",
    "    checkpoint = torch.load(CKPT_PATH, map_location=device, weights_only=False)\n",
    "    \n",
    "    # We only need the 'model_state_dict' for inference. \n",
    "    # The optimizer states are only needed if we plan to resume training.\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model loaded successfully (from Epoch {checkpoint['epoch']+1})\")\n",
    "else:\n",
    "    print(f\"Warning: No checkpoint found at {CKPT_PATH}. Using random initialization.\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, seq_len, num_steps=100, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Generate text using the reverse diffusion process.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained MDLM\n",
    "        seq_len: Length of sequence to generate\n",
    "        num_steps: Number of denoising steps\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Generated token sequence\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Start with all masks\n",
    "    x = torch.full((1, seq_len), MASK_TOKEN, dtype=torch.long, device=device)\n",
    "    \n",
    "    # Linearly spaced time steps from 1 to 0\n",
    "    timesteps = torch.linspace(1, 0, num_steps + 1, device=device)\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        t_current = timesteps[i]\n",
    "        t_next = timesteps[i + 1]\n",
    "        \n",
    "        # Get model predictions\n",
    "        t_batch = torch.tensor([t_current], device=device)\n",
    "        logits = model(x, t_batch)  # [1, seq_len, vocab_size]\n",
    "        # Model is predicting all positions at each forward pass\n",
    "        \n",
    "        # Don't predict [MASK] token during sampling\n",
    "        logits[:, :, MASK_TOKEN] = float('-inf')\n",
    "        \n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)  # [1, seq_len, vocab_size]\n",
    "        \n",
    "        # Sample predictions for all positions\n",
    "        # (we'll only use some based on remasking strategy)\n",
    "        pred_tokens = torch.multinomial(\n",
    "            probs.view(-1, probs.size(-1)), \n",
    "            num_samples=1\n",
    "        ).view(1, seq_len)\n",
    "        \n",
    "        # Determine which positions to unmask this step (Ancestral Sampling)\n",
    "        # Instead of forcing a specific count (top-k), we roll a probability die for each token\n",
    "        \n",
    "        # 1. Calculate the probability of unmasking at this step\n",
    "        if t_current > 0:\n",
    "            p_unmask = (t_current - t_next) / t_current\n",
    "        else:\n",
    "            p_unmask = 1.0  # Force finish at the very end\n",
    "            \n",
    "        # 2. Roll the dice for every position\n",
    "        # Generate random values [0, 1]. If value < p_unmask, we reveal the token.\n",
    "        random_values = torch.rand_like(probs[:, :, 0]) # Shape: [1, seq_len]\n",
    "        should_unmask = random_values < p_unmask\n",
    "        \n",
    "        # 3. Apply updates\n",
    "        # Only update if it is CURRENTLY a mask AND the dice roll said \"Unmask\"\n",
    "        is_mask = (x == MASK_TOKEN)\n",
    "        update_mask = is_mask & should_unmask\n",
    "        \n",
    "        x[update_mask] = pred_tokens[update_mask]\n",
    "    \n",
    "    # Final cleanup: unmask any remaining masks\n",
    "    is_mask = (x == MASK_TOKEN)\n",
    "    if is_mask.any():\n",
    "        logits = model(x, torch.tensor([0.0], device=device))\n",
    "        logits[:, :, MASK_TOKEN] = float('-inf')\n",
    "        probs = F.softmax(logits / temperature, dim=-1)\n",
    "        final_preds = probs.argmax(dim=-1)\n",
    "        x[is_mask] = final_preds[is_mask]\n",
    "    \n",
    "    return x[0].tolist()\n",
    "\n",
    "# Generate some samples!\n",
    "print(\"Generated samples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tokens = sample(model, seq_len=512, num_steps=100, temperature=1.0)\n",
    "text = decode(tokens)\n",
    "print(''.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ed5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
